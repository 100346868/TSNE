epsilon = 500
min_gain = .01
initial_P_gain = 4
eps = 2^(-52) # typical machine precision
if (!is.null(initial_config) && is.matrix(initial_config)) {
if (nrow(initial_config) != n | ncol(initial_config) != k){
stop('initial_config argument does not match necessary configuration for X')
}
ydata = initial_config
initial_P_gain = 1
} else {
ydata = matrix(rnorm(k * n),n)
}
P = .x2p(X,perplexity, 1e-5)$P
P = .5 * (P + t(P))
P[P < eps]<-eps
P = P/sum(P)
P = P * initial_P_gain
grads =  matrix(0,nrow(ydata),ncol(ydata))
incs =  matrix(0,nrow(ydata),ncol(ydata))
gains = matrix(1,nrow(ydata),ncol(ydata))
for (iter in 1:max_iter){
if (iter %% epoch == 0) { # epoch
cost =  sum(apply(P * log((P+eps)/(Q+eps)),1,sum))
message("Epoch: Iteration #",iter," error is: ",cost)
if (cost < min_cost) break
if (!is.null(epoch_callback)) epoch_callback(ydata)
}
sum_ydata = apply(ydata^2, 1, sum)
num =  1/(1 + sum_ydata +    sweep(-2 * ydata %*% t(ydata),2, -t(sum_ydata)))
diag(num)=0
Q = num / sum(num)
if (any(is.nan(num))) message ('NaN in grad. descent')
Q[Q < eps] = eps
stiffnesses = 4 * (P-Q) * num
for (i in 1:n){
grads[i,] = apply(sweep(-ydata, 2, -ydata[i,]) * stiffnesses[,i],2,sum)
}
gains = ((gains + .2) * abs(sign(grads) != sign(incs)) +
gains * .8 * abs(sign(grads) == sign(incs)))
gains[gains < min_gain] = min_gain
incs = momentum * incs - epsilon * (gains * grads)
ydata = ydata + incs
ydata = sweep(ydata,2,apply(ydata,2,mean))
if (iter == mom_switch_iter) momentum = final_momentum
if (iter == 100 && is.null(initial_config)) P = P/4
}
ydata
#Firstly we process the data
data = iris
x =data
n = nrow(data)
p = ncol(data)
#set initial parameters
learningrate = 0.5
momentum  =0.5
perplexity = 5
#Set initial configuration of the low dimensional
#data
#number of dimensions we want to reduce to.
sigma = 1/sqrt(2)
q = 2
ydata = matrix(data = rnorm(n = (n * p), mean = 0, sd = sigma),
nrow = n, ncol = q)
#calculating the conditional probabilities in high dimension:
P = matrix(data = 0, nrow = n, ncol = p)
P = as.matrix(exp((-dist(x, diag = TRUE)^2)/2*sigma^2))
v = rowSums(P) - 1
p_icondj = P * v
p_ij = (p_icondj + t(p_icondj))/2*n
p_jcondi = t(p_icondj)
perplexity = 2^(-colSums(p_jcondi*log2(p_jcondi)))
#calculating the conditional probabilities in low dimension:
Q = matrix(data = 0, nrow = n, ncol = p)
Q = as.matrix(exp((-dist(ydata, diag = TRUE)^2)))
#The 1 comes from the diagonal
v2 = rowSums(Q) - 1
q_icondj = Q * v2
q_ij = (q_icondj + t(q_icondj))/2*n
q_jcondi = t(q_icondj)
inversa = as.matrix((1+ abs(dist(ydata, diag = TRUE))^2)^-1)
#Computing the gradient
gradient = 4*colSums( (p_ij - q_ij)*(as.matrix(dist(ydata,diag= TRUE)))*inversa )
#Once we have computed everything:
T = vector(length = 1000)
Ycalc = matrix(data = ydata,nrow = row(ydata) ,ncol = col(ydata))
Ycalc1 = matrix(data = ydata,nrow = row(ydata) ,ncol = col(ydata))
Ycalc2 = matrix(data = ydata,nrow = row(ydata) ,ncol = col(ydata))
#gamma1 is gamma in the previous step
#gamma2 is gamma 2 steps before
for (t in T) {
Ycalc = Ycalc1 + learningrate*gradient + momentum*(Ycalc1 - Ycalc2)
Ycalc2 = Ycalc1
Ycalc1 = Ycalc
plot(Ycalc)
Ycalc
}
data = iris[,-5]
x =data
n = nrow(data)
p = ncol(data)
#set initial parameters
learningrate = 0.5
momentum  =0.5
perplexity = 5
#Set initial configuration of the low dimensional
#data
#number of dimensions we want to reduce to.
sigma = 1/sqrt(2)
q = 2
ydata = matrix(data = rnorm(n = (n * p), mean = 0, sd = sigma),
nrow = n, ncol = q)
#calculating the conditional probabilities in high dimension:
P = matrix(data = 0, nrow = n, ncol = p)
P = as.matrix(exp((-dist(x, diag = TRUE)^2)/2*sigma^2))
v = rowSums(P) - 1
p_icondj = P * v
p_ij = (p_icondj + t(p_icondj))/2*n
p_jcondi = t(p_icondj)
perplexity = 2^(-colSums(p_jcondi*log2(p_jcondi)))
#calculating the conditional probabilities in low dimension:
Q = matrix(data = 0, nrow = n, ncol = p)
Q = as.matrix(exp((-dist(ydata, diag = TRUE)^2)))
#The 1 comes from the diagonal
v2 = rowSums(Q) - 1
q_icondj = Q * v2
q_ij = (q_icondj + t(q_icondj))/2*n
q_jcondi = t(q_icondj)
inversa = as.matrix((1+ abs(dist(ydata, diag = TRUE))^2)^-1)
#Computing the gradient
gradient = 4*colSums( (p_ij - q_ij)*(as.matrix(dist(ydata,diag= TRUE)))*inversa )
#Once we have computed everything:
T = vector(length = 1000)
Ycalc = matrix(data = ydata,nrow = row(ydata) ,ncol = col(ydata))
Ycalc1 = matrix(data = ydata,nrow = row(ydata) ,ncol = col(ydata))
Ycalc2 = matrix(data = ydata,nrow = row(ydata) ,ncol = col(ydata))
#gamma1 is gamma in the previous step
#gamma2 is gamma 2 steps before
for (t in T) {
Ycalc = Ycalc1 + learningrate*gradient + momentum*(Ycalc1 - Ycalc2)
Ycalc2 = Ycalc1
Ycalc1 = Ycalc
message("Epoch: Iteration #",t)
plot(Ycalc)
Ycalc
}
for (t in 1000) {
Ycalc = Ycalc1 + learningrate*gradient + momentum*(Ycalc1 - Ycalc2)
Ycalc2 = Ycalc1
Ycalc1 = Ycalc
message("Epoch: Iteration #",t)
plot(Ycalc)
Ycalc
}
for (t in 1000) {
Ycalc = Ycalc1 + learningrate*gradient + momentum*(Ycalc1 - Ycalc2)
Ycalc2 = Ycalc1
Ycalc1 = Ycalc
message("Epoch: Iteration #",t)
plot(Ycalc)
Ycalc
}
Te = matrix(data = 0,nrow = 1000,ncol = 0)
for (t in Te) {
Ycalc = Ycalc1 + learningrate*gradient + momentum*(Ycalc1 - Ycalc2)
Ycalc2 = Ycalc1
Ycalc1 = Ycalc
message("Epoch: Iteration #",t)
plot(Ycalc)
Ycalc
}
#Firstly we process the data
data = iris[,-5]
x =data
n = nrow(data)
p = ncol(data)
#set initial parameters
learningrate = 0.5
momentum  =0.5
perplexity = 5
View(data)
View(data)
#Set initial configuration of the low dimensional
#data
#number of dimensions we want to reduce to.
sigma = 1/sqrt(2)
q = 2
ydata = matrix(data = rnorm(n = (n * p), mean = 0, sd = sigma),
nrow = n, ncol = q)
#calculating the conditional probabilities in high dimension:
P = matrix(data = 0, nrow = n, ncol = p)
P = as.matrix(exp((-dist(x, diag = TRUE)^2)/2*sigma^2))
v = rowSums(P) - 1
p_icondj = P * v
p_ij = (p_icondj + t(p_icondj))/2*n
p_jcondi = t(p_icondj)
perplexity = 2^(-colSums(p_jcondi*log2(p_jcondi)))
#calculating the conditional probabilities in low dimension:
Q = matrix(data = 0, nrow = n, ncol = p)
Q = as.matrix(exp((-dist(ydata, diag = TRUE)^2)))
#The 1 comes from the diagonal
v2 = rowSums(Q) - 1
q_icondj = Q * v2
q_ij = (q_icondj + t(q_icondj))/2*n
q_jcondi = t(q_icondj)
inversa = as.matrix((1+ abs(dist(ydata, diag = TRUE))^2)^-1)
#Computing the gradient
gradient = 4*colSums( (p_ij - q_ij)*(as.matrix(dist(ydata,diag= TRUE)))*inversa )
gradient = 4*colSums( (p_ij - q_ij)*(as.matrix(dist(ydata,diag= TRUE)))*inversa )
perplexity = 2^(-colSums(p_jcondi*log2(p_jcondi)))
? load
load(Admission_Predict.csv)
X = load(Admission_Predict.csv)
X = read.csv(Admission_Predict.csv ,header = TRUE)
X = read.csv(file = Admission_Predict.csv ,header = TRUE)
X = read.csv(file = Admission_Predict.csv, header = TRUE)
X = read.csv(file = "Admission_Predict.csv", header = TRUE)
data = X
n = nrow(data)
p = ncol(data)
#set initial parameters
learningrate = 0.5
momentum  =0.5
perplexity = 5
#Set initial configuration of the low dimensional
#data
#number of dimensions we want to reduce to.
sigma = 1/sqrt(2)
q = 2
ydata = matrix(data = rnorm(n = (n * p), mean = 0, sd = sigma),
nrow = n, ncol = q)
#calculating the conditional probabilities in high dimension:
P = matrix(data = 0, nrow = n, ncol = p)
P = as.matrix(exp((-dist(x, diag = TRUE)^2)/2*sigma^2))
v = rowSums(P) - 1
p_icondj = P * v
p_ij = (p_icondj + t(p_icondj))/2*n
p_jcondi = t(p_icondj)
perplexity = 2^(-colSums(p_jcondi*log2(p_jcondi)))
#calculating the conditional probabilities in low dimension:
Q = matrix(data = 0, nrow = n, ncol = p)
Q = as.matrix(exp((-dist(ydata, diag = TRUE)^2)))
#The 1 comes from the diagonal
v2 = rowSums(Q) - 1
q_icondj = Q * v2
q_ij = (q_icondj + t(q_icondj))/2*n
q_jcondi = t(q_icondj)
inversa = as.matrix((1+ abs(dist(ydata, diag = TRUE))^2)^-1)
#Computing the gradient
gradient = 4*colSums( (p_ij - q_ij)*(as.matrix(dist(ydata,diag= TRUE)))*inversa )
#Once we have computed everything:
Ycalc = matrix(data = ydata,nrow = row(ydata) ,ncol = col(ydata))
Ycalc1 = matrix(data = ydata,nrow = row(ydata) ,ncol = col(ydata))
Ycalc2 = matrix(data = ydata,nrow = row(ydata) ,ncol = col(ydata))
#gamma1 is gamma in the previous step
#gamma2 is gamma 2 steps before
Te = matrix(data = 0,nrow = 1000,ncol = 0)
for (t in Te) {
Ycalc = Ycalc1 + learningrate*gradient + momentum*(Ycalc1 - Ycalc2)
Ycalc2 = Ycalc1
Ycalc1 = Ycalc
message("Epoch: Iteration #",t)
plot(Ycalc)
Ycalc
}
X = read.csv("Admission_Predict.csv",header=false)
X = read.csv("Admission_Predict.csv",header=FALSE)
data = x
data = X
n = nrow(data)
p = ncol(data)
#set initial parameters
learningrate = 0.5
momentum  =0.5
perplexity = 5
#Set initial configuration of the low dimensional
#data
#number of dimensions we want to reduce to.
sigma = 1/sqrt(2)
q = 2
ydata = matrix(data = rnorm(n = (n * p), mean = 0, sd = sigma),
nrow = n, ncol = q)
#calculating the conditional probabilities in high dimension:
P = matrix(data = 0, nrow = n, ncol = p)
P = as.matrix(exp((-dist(x, diag = TRUE)^2)/2*sigma^2))
v = rowSums(P) - 1
p_icondj = P * v
p_ij = (p_icondj + t(p_icondj))/2*n
p_jcondi = t(p_icondj)
perplexity = 2^(-colSums(p_jcondi*log2(p_jcondi)))
#calculating the conditional probabilities in low dimension:
Q = matrix(data = 0, nrow = n, ncol = p)
Q = as.matrix(exp((-dist(ydata, diag = TRUE)^2)))
#The 1 comes from the diagonal
v2 = rowSums(Q) - 1
q_icondj = Q * v2
q_ij = (q_icondj + t(q_icondj))/2*n
q_jcondi = t(q_icondj)
inversa = as.matrix((1+ abs(dist(ydata, diag = TRUE))^2)^-1)
#Computing the gradient
gradient = 4*colSums( (p_ij - q_ij)*(as.matrix(dist(ydata,diag= TRUE)))*inversa )
#Once we have computed everything:
Ycalc = matrix(data = ydata,nrow = row(ydata) ,ncol = col(ydata))
Ycalc1 = matrix(data = ydata,nrow = row(ydata) ,ncol = col(ydata))
Ycalc2 = matrix(data = ydata,nrow = row(ydata) ,ncol = col(ydata))
#gamma1 is gamma in the previous step
#gamma2 is gamma 2 steps before
Te = matrix(data = 0,nrow = 1000,ncol = 0)
for (t in Te) {
Ycalc = Ycalc1 + learningrate*gradient + momentum*(Ycalc1 - Ycalc2)
Ycalc2 = Ycalc1
Ycalc1 = Ycalc
message("Epoch: Iteration #",t)
plot(Ycalc)
Ycalc
}
perplexity = 2^(-colSums(p_jcondi*log2(p_jcondi)))
p_jcondi = t(p_icondj)
View(p_jcondi)
perplexity = 2^(-colSums(p_jcondi*log2(p_jcondi)))
perplex = 2^(-sum(p_jcondi*log2(p_jcondi)))
log2(p_jcondi)
p_jcondi
sum(p_jcondi*log2(p_jcondi))
colSums(p_jcondi*log2(p_jcondi))
log2(p_jcondi)
p_jcondi
perplex = 2^(-sum(p_jcondi%*%log2(p_jcondi)))
perplex = 2^(-sum(p_jcondi%*%log2(p_jcondi)))
-sum(p_jcondi%*%log2(p_jcondi))
p_jcondi%*%log2(p_jcondi)
p_jcondi*log2(p_jcondi)
rowSums(p_jcondi*log2(p_jcondi))
cost = rowsum(colSums((p_ij*log(p_ij/q_ij))))
X = read.csv(file = "Admission_Predict.csv", header = FALSE)
X
data = X
n = nrow(data)
p = ncol(data)
#set initial parameters
learningrate = 0.5
momentum  =0.5
perplexity = 5
#Set initial configuration of the low dimensional
#data
#number of dimensions we want to reduce to.
sigma = 1/sqrt(2)
q = 2
ydata = matrix(data = rnorm(n = (n * p), mean = 0, sd = sigma),
nrow = n, ncol = q)
#calculating the conditional probabilities in high dimension:
P = matrix(data = 0, nrow = n, ncol = p)
P = as.matrix(exp((-dist(x, diag = TRUE)^2)/2*sigma^2))
v = rowSums(P) - 1
p_icondj = P * v
p_ij = (p_icondj + t(p_icondj))/2*n
p_jcondi = t(p_icondj)
perplexity = 2^(-colSums(p_jcondi*log2(p_jcondi)))
#calculating the conditional probabilities in low dimension:
Q = matrix(data = 0, nrow = n, ncol = p)
Q = as.matrix(exp((-dist(ydata, diag = TRUE)^2)))
#The 1 comes from the diagonal
v2 = rowSums(Q) - 1
q_icondj = Q * v2
q_ij = (q_icondj + t(q_icondj))/2*n
q_jcondi = t(q_icondj)
cost = rowsum(colSums((p_ij*log(p_ij/q_ij))))
inversa = as.matrix((1+ abs(dist(ydata, diag = TRUE))^2)^-1)
#Computing the gradient
gradient = 4*colSums( (p_ij - q_ij)*(as.matrix(dist(ydata,diag= TRUE)))*inversa )
#Once we have computed everything:
Ycalc = matrix(data = ydata,nrow = row(ydata) ,ncol = col(ydata))
Ycalc1 = matrix(data = ydata,nrow = row(ydata) ,ncol = col(ydata))
Ycalc2 = matrix(data = ydata,nrow = row(ydata) ,ncol = col(ydata))
x = data
n = nrow(data)
p = ncol(data)
#set initial parameters
learningrate = 0.5
momentum  =0.5
perplexity = 5
#Set initial configuration of the low dimensional
#data
#number of dimensions we want to reduce to.
sigma = 1/sqrt(2)
q = 2
ydata = matrix(data = rnorm(n = (n * p), mean = 0, sd = sigma),
nrow = n, ncol = q)
#calculating the conditional probabilities in high dimension:
P = matrix(data = 0, nrow = n, ncol = p)
P = as.matrix(exp((-dist(x, diag = TRUE)^2)/2*sigma^2))
v = rowSums(P) - 1
p_icondj = P * v
p_ij = (p_icondj + t(p_icondj))/2*n
p_jcondi = t(p_icondj)
perplexity = 2^(-colSums(p_jcondi*log2(p_jcondi)))
#calculating the conditional probabilities in low dimension:
Q = matrix(data = 0, nrow = n, ncol = p)
Q = as.matrix(exp((-dist(ydata, diag = TRUE)^2)))
#The 1 comes from the diagonal
v2 = rowSums(Q) - 1
q_icondj = Q * v2
q_ij = (q_icondj + t(q_icondj))/2*n
q_jcondi = t(q_icondj)
cost = rowsum(colSums((p_ij*log(p_ij/q_ij))))
inversa = as.matrix((1+ abs(dist(ydata, diag = TRUE))^2)^-1)
#Computing the gradient
gradient = 4*colSums( (p_ij - q_ij)*(as.matrix(dist(ydata,diag= TRUE)))*inversa )
#Once we have computed everything:
Ycalc = matrix(data = ydata,nrow = row(ydata) ,ncol = col(ydata))
Ycalc1 = matrix(data = ydata,nrow = row(ydata) ,ncol = col(ydata))
Ycalc2 = matrix(data = ydata,nrow = row(ydata) ,ncol = col(ydata))
#gamma1 is gamma in the previous step
#gamma2 is gamma 2 steps before
Te = matrix(data = 0,nrow = 1000,ncol = 0)
cost = rowsum(colSums((p_ij*log(p_ij/q_ij))))
suma = colSums((p_ij*log(p_ij/q_ij)))
View(p_ij)
suma = colSums(as.matrix(p_ij*log(p_ij/q_ij)))
log(p_ij/q_ij)
log(p_ij%/%q_ij)
p_ij
ydata = matrix(data = rnorm(n = (n * p), mean = 0, sd = sigma),
nrow = n, ncol = q)
data = iris
x =data
n = nrow(data)
p = ncol(data)
#set initial parameters
learningrate = 0.5
momentum  =0.5
perplexity = 5
sigma = 1/sqrt(2)
q = 2
ydata = matrix(data = rnorm(n = (n * p), mean = 0, sd = sigma),
nrow = n, ncol = q)
P = matrix(data = 0, nrow = n, ncol = p)
P = as.matrix(exp((-dist(x, diag = TRUE)^2)/2*sigma^2))
v = rowSums(P) - 1
P = as.matrix(exp((-dist(x, diag = TRUE)^2)/2*sigma^2))
v = rowSums(P) - 1
View(P)
p_icondj = P * v
p_ij = (p_icondj + t(p_icondj))/2*n
p_jcondi = t(p_icondj)
perplexity = 2^(-colSums(p_jcondi*log2(p_jcondi)))
colSums(p_jcondi*log2(p_jcondi))
colSums(p_jcondi%*%log2(p_jcondi))
uno = p_jcondi*log2(p_jcondi)
View(uno)
entropy = -colSums(as.matrix(x = uno,diag = TRUE))
entropy = -colSums(as.matrix(x = uno,diag = FALSE))
p_jcondi*log2(p_jcondi) - 1
entropy = -colSums(uno)
data = iris
n = nrow(data)
p = ncol(data)
#set initial parameters
learningrate = 0.5
momentum  =0.5
perplexity = 5
#Set initial configuration of the low dimensional
#data
#number of dimensions we want to reduce to.
sigma = 1/sqrt(2)
q = 2
ydata = matrix(data = rnorm(n = (n * p), mean = 0, sd = sigma),
nrow = n, ncol = q)
#calculating the conditional probabilities in high dimension:
P = matrix(data = 0, nrow = n, ncol = p)
P = as.matrix(exp((-dist(x, diag = TRUE)^2)/2*sigma^2))
v = rowSums(P) - 1
p_icondj = P * v
p_ij = (p_icondj + t(p_icondj))/2*n
p_jcondi = t(p_icondj)
p_jcondi*log2(p_jcondi)
log2(p_jcondi)
p_jcondi
data = iris
x =data
n = nrow(data)
p = ncol(data)
#set initial parameters
learningrate = 0.5
momentum  =0.5
perplexity = 5
#Set initial configuration of the low dimensional
#data
#number of dimensions we want to reduce to.
sigma = 1/sqrt(2)
q = 2
ydata = matrix(data = rnorm(n = (n * p), mean = 0, sd = sigma),
nrow = n, ncol = q)
#calculating the conditional probabilities in high dimension:
P = matrix(data = 0, nrow = n, ncol = p)
P = as.matrix(exp((-dist(x, diag = TRUE)^2)/2*sigma^2))
v = rowSums(P) - 1
p_icondj = P * v
p_ij = (p_icondj + t(p_icondj))/2*n
p_jcondi = t(p_icondj)
p_jcondi
p_jcondi
log2(p_jcondi)
p_jcondi*log2(p_jcondi)
uno = p_jcondi*log2(p_jcondi)
uno = p_jcondi*log2(p_jcondi) - 1
entropy = -colSums(uno) - 1
uno = p_jcondi*log2(p_jcondi)
View(uno)
? as.matrix
m1 = as.matrix(x = uno, diag = FALSE)
m1 = as.matrix(x = uno, diag = TRUE)
