#Dejo directamente la del comentario
Ycalc =  ydata
Ycalc1 = ydata
Ycalc2 = ydata
#Ycalc1 is Ycalc in the previous step
#ycalc2 is Ycalc 2 steps before
#Te = vector(mode = "list", length = 100)
Te = as.matrix(1:1000)
for (t in Te) {
for (i in 1:n){
gradient[i,] = apply(sweep(-Ycalc, 2, -Ycalc[i,]) * stiffnesses[,i] ,2,sum)
}
Ycalc = Ycalc1 + learningrate * gradient + momentum * (Ycalc1 - Ycalc2)
#Ycalc = Ycalc1 + 100 * gradient + 0.8 * (Ycalc1 - Ycalc2)
#learningrate = learningrate +1;
Ycalc2 = Ycalc1
Ycalc1 = Ycalc
message("Iteration #",t)
}
Ycalc
return(Ycalc)
#Como resultado final se está agrupando pero se está agrupando
#todo en el mismo sitio, no se separa en grupos.
}
tsne1 <- tsne(data)
plot(tsne1, pch = 15, col = iris$Species)
#Cargar la data = iris[,-5]
#Sin la columna de las especiess(nombres)
tsne<- function(data){
x = data
n = nrow(data)
p = ncol(data)
#set initial parameters
learningrate = 100
#Establecemos estos valores iniciales
momentum  = 0.5
#momentum = 0.8
perplexity = 10
#Set initial configuration of the low dimensional
#data
#number of dimensions we want to reduce to.
sigma = 1/sqrt(2)
q = 2
ydata = matrix(data = rnorm(n = (n * p), mean = 0, sd = sigma), nrow = n, ncol = q)
#calculating the conditional probabilities in high dimension:
P = matrix(data = 0, nrow = n, ncol = p)
P = as.matrix(exp((-dist(x, diag = TRUE) ^ 2) / (2 * sigma ^ 2)))
#exceptuating the diagonal
v = rowSums(P) - 1
p_icondj = P / v
p_ij = (p_icondj + t(p_icondj)) / (2 * n)
p_jcondi = t(p_icondj)
# I have found a trouble computing the perplexity:
# Main trouble is computing the log of 0, since the diag is 0
uno = (p_jcondi) * log2(p_jcondi + 1)
entropy = -colSums(uno)
perplexity = 2 ^ (entropy)
#Este valor deberia dar entre 5 y 50 para un correcto desarrollo
perplexity = 30
#calculating the conditional probabilities in low dimension:
Q = matrix(data = 0, nrow = n, ncol = p)
Q = as.matrix(exp((-dist(ydata, diag = TRUE) ^ 2)))
#The 1 comes from the diagonal
v2 = rowSums(Q) - 1
q_icondj = Q / v2
q_ij = (q_icondj + t(q_icondj)) / (2 * n)
q_jcondi = t(q_icondj)
#Computing the gradient
#Para hacer la inversa de esta matriz utilizo la funcion solve
inversa = as.matrix((1+ (dist(ydata, diag = TRUE)) ^ 2))
inversa = solve(inversa)
#ARREGLAR EL GRADIENTE
gradient = matrix(data = 0, nrow = n, ncol = q)
# tot = vector(mode = "list", length = n)
# for(i in tot) {
#  gradient[i,] = 4 * colSums( (p_ij - q_ij) * (ydata - ydata[,i]) * inversa)
# }
#Probar de esta forma
#apply con 1 es row
#aaply con 2 es column
stiffnesses = 4 * (p_ij - q_ij) * inversa
#El problema está en esta funcion
#No actualiza bien el gradiente de manera que no se repelen bien los puntos
#Para coger las columnas y filas de ydata:
#Mirar documentacion pero como en matlab: ydata[1,]
#Once we have computed everything:
#Dejo directamente la del comentario
Y_t =  ydata
Y_t_1 = ydata
Y_t_2 = ydata
#Ycalc1 is Ycalc in the previous step
#ycalc2 is Ycalc 2 steps before
#Te = vector(mode = "list", length = 100)
Te = as.matrix(1:1000)
for (t in Te) {
for (i in 1:n){
gradient[i,] = apply(sweep(-Y_t, 2, -Y_t[i,]) * stiffnesses[,i] ,2,sum)
}
Y_t = Y_t_1 + learningrate * gradient + momentum * (Y_t_1 - Y_t_2)
#Ycalc = Ycalc1 + 100 * gradient + 0.8 * (Ycalc1 - Ycalc2)
#learningrate = learningrate +1;
Y_t_2 = Y_t_1
Y_t_1 = Y_t
message("Iteration #",t)
if((i%%10)==0){
plot(Y_t, pch =15 , col = iris$Species)
}
}
Y_t
return(Y_t)
#Como resultado final se está agrupando pero se está agrupando
#todo en el mismo sitio, no se separa en grupos.
}
tsne1 <- tsne(data)
#Cargar la data = iris[,-5]
#Sin la columna de las especiess(nombres)
tsne<- function(data){
x = data
n = nrow(data)
p = ncol(data)
#set initial parameters
learningrate = 100
#Establecemos estos valores iniciales
momentum  = 0.5
#momentum = 0.8
perplexity = 10
#Set initial configuration of the low dimensional
#data
#number of dimensions we want to reduce to.
sigma = 1/sqrt(2)
q = 2
ydata = matrix(data = rnorm(n = (n * p), mean = 0, sd = sigma), nrow = n, ncol = q)
#calculating the conditional probabilities in high dimension:
P = matrix(data = 0, nrow = n, ncol = p)
P = as.matrix(exp((-dist(x, diag = TRUE) ^ 2) / (2 * sigma ^ 2)))
#exceptuating the diagonal
v = rowSums(P) - 1
p_icondj = P / v
p_ij = (p_icondj + t(p_icondj)) / (2 * n)
p_jcondi = t(p_icondj)
# I have found a trouble computing the perplexity:
# Main trouble is computing the log of 0, since the diag is 0
uno = (p_jcondi) * log2(p_jcondi + 1)
entropy = -colSums(uno)
perplexity = 2 ^ (entropy)
#Este valor deberia dar entre 5 y 50 para un correcto desarrollo
perplexity = 30
#calculating the conditional probabilities in low dimension:
Q = matrix(data = 0, nrow = n, ncol = p)
Q = as.matrix(exp((-dist(ydata, diag = TRUE) ^ 2)))
#The 1 comes from the diagonal
v2 = rowSums(Q) - 1
q_icondj = Q / v2
q_ij = (q_icondj + t(q_icondj)) / (2 * n)
q_jcondi = t(q_icondj)
#Computing the gradient
#Para hacer la inversa de esta matriz utilizo la funcion solve
inversa = as.matrix((1+ (dist(ydata, diag = TRUE)) ^ 2))
inversa = solve(inversa)
#ARREGLAR EL GRADIENTE
gradient = matrix(data = 0, nrow = n, ncol = q)
# tot = vector(mode = "list", length = n)
# for(i in tot) {
#  gradient[i,] = 4 * colSums( (p_ij - q_ij) * (ydata - ydata[,i]) * inversa)
# }
#Probar de esta forma
#apply con 1 es row
#aaply con 2 es column
stiffnesses = 4 * (p_ij - q_ij) * inversa
#El problema está en esta funcion
#No actualiza bien el gradiente de manera que no se repelen bien los puntos
#Para coger las columnas y filas de ydata:
#Mirar documentacion pero como en matlab: ydata[1,]
#Once we have computed everything:
#Dejo directamente la del comentario
Y_t =  ydata
Y_t_1 = ydata
Y_t_2 = ydata
#Ycalc1 is Ycalc in the previous step
#ycalc2 is Ycalc 2 steps before
#Te = vector(mode = "list", length = 100)
Te = as.matrix(1:1000)
for (t in Te) {
for (i in 1:n){
gradient[i,] = apply(sweep(-Y_t, 2, -Y_t[i,]) * stiffnesses[,i] ,2,sum)
}
Y_t = Y_t_1 + learningrate * gradient + momentum * (Y_t_1 - Y_t_2)
#Ycalc = Ycalc1 + 100 * gradient + 0.8 * (Ycalc1 - Ycalc2)
learningrate = learningrate +1;
Y_t_2 = Y_t_1
Y_t_1 = Y_t
message("Iteration #",t)
if((i%%10)==0){
plot(Y_t, pch =15 , col = iris$Species)
}
}
Y_t
return(Y_t)
#Como resultado final se está agrupando pero se está agrupando
#todo en el mismo sitio, no se separa en grupos.
}
#Cargar la data = iris[,-5]
#Sin la columna de las especiess(nombres)
tsne<- function(data){
x = data
n = nrow(data)
p = ncol(data)
#set initial parameters
learningrate = 100
#Establecemos estos valores iniciales
momentum  = 0.5
#momentum = 0.8
perplexity = 10
#Set initial configuration of the low dimensional
#data
#number of dimensions we want to reduce to.
sigma = 1/sqrt(2)
q = 2
ydata = matrix(data = rnorm(n = (n * p), mean = 0, sd = sigma), nrow = n, ncol = q)
#calculating the conditional probabilities in high dimension:
P = matrix(data = 0, nrow = n, ncol = p)
P = as.matrix(exp((-dist(x, diag = TRUE) ^ 2) / (2 * sigma ^ 2)))
#exceptuating the diagonal
v = rowSums(P) - 1
p_icondj = P / v
p_ij = (p_icondj + t(p_icondj)) / (2 * n)
p_jcondi = t(p_icondj)
# I have found a trouble computing the perplexity:
# Main trouble is computing the log of 0, since the diag is 0
uno = (p_jcondi) * log2(p_jcondi + 1)
entropy = -colSums(uno)
perplexity = 2 ^ (entropy)
#Este valor deberia dar entre 5 y 50 para un correcto desarrollo
perplexity = 30
#calculating the conditional probabilities in low dimension:
Q = matrix(data = 0, nrow = n, ncol = p)
Q = as.matrix(exp((-dist(ydata, diag = TRUE) ^ 2)))
#The 1 comes from the diagonal
v2 = rowSums(Q) - 1
q_icondj = Q / v2
q_ij = (q_icondj + t(q_icondj)) / (2 * n)
q_jcondi = t(q_icondj)
#Computing the gradient
#Para hacer la inversa de esta matriz utilizo la funcion solve
inversa = as.matrix((1+ (dist(ydata, diag = TRUE)) ^ 2))
inversa = solve(inversa)
#ARREGLAR EL GRADIENTE
gradient = matrix(data = 0, nrow = n, ncol = q)
# tot = vector(mode = "list", length = n)
# for(i in tot) {
#  gradient[i,] = 4 * colSums( (p_ij - q_ij) * (ydata - ydata[,i]) * inversa)
# }
#Probar de esta forma
#apply con 1 es row
#aaply con 2 es column
stiffnesses = 4 * (p_ij - q_ij) * inversa
#El problema está en esta funcion
#No actualiza bien el gradiente de manera que no se repelen bien los puntos
#Para coger las columnas y filas de ydata:
#Mirar documentacion pero como en matlab: ydata[1,]
#Once we have computed everything:
#Dejo directamente la del comentario
Y_t =  ydata
Y_t_1 = ydata
Y_t_2 = ydata
#Ycalc1 is Ycalc in the previous step
#ycalc2 is Ycalc 2 steps before
#Te = vector(mode = "list", length = 100)
Te = as.matrix(1:1000)
for (t in Te) {
for (i in 1:n){
gradient[i,] = apply(sweep(-Y_t, 2, -Y_t[i,]) * stiffnesses[,i] ,2,sum)
}
Y_t = Y_t_1 + learningrate * gradient + momentum * (Y_t_1 - Y_t_2)
#Ycalc = Ycalc1 + 100 * gradient + 0.8 * (Ycalc1 - Ycalc2)
learningrate = learningrate +1;
Y_t_2 = Y_t_1
Y_t_1 = Y_t
message("Iteration #",t)
if((i%%10)==0){
plot(Y_t, pch =15 , col = iris$Species)
}
}
Y_t
return(Y_t)
#Como resultado final se está agrupando pero se está agrupando
#todo en el mismo sitio, no se separa en grupos.
}
data <- iris[,-5]
tsne1 <- tsne(data)
data <- iris[,1:2]
x = data
n = nrow(data)
p = ncol(data)
#set initial parameters
learningrate = 100
#Establecemos estos valores iniciales
momentum  = 0.5
#momentum = 0.8
perplexity = 10
#Set initial configuration of the low dimensional
#data
#number of dimensions we want to reduce to.
sigma = 1/sqrt(2)
q = 2
ydata = matrix(data = rnorm(n = (n * p), mean = 0, sd = sigma), nrow = n, ncol = q)
#calculating the conditional probabilities in high dimension:
P = matrix(data = 0, nrow = n, ncol = p)
P = as.matrix(exp((-dist(x, diag = TRUE) ^ 2) / (2 * sigma ^ 2)))
#exceptuating the diagonal
v = rowSums(P) - 1
p_icondj = P / v
p_ij = (p_icondj + t(p_icondj)) / (2 * n)
p_jcondi = t(p_icondj)
# I have found a trouble computing the perplexity:
# Main trouble is computing the log of 0, since the diag is 0
uno = (p_jcondi) * log2(p_jcondi + 1)
entropy = -colSums(uno)
perplexity = 2 ^ (entropy)
#Este valor deberia dar entre 5 y 50 para un correcto desarrollo
perplexity = 30
#calculating the conditional probabilities in low dimension:
Q = matrix(data = 0, nrow = n, ncol = p)
Q = as.matrix(exp((-dist(ydata, diag = TRUE) ^ 2)))
#The 1 comes from the diagonal
v2 = rowSums(Q) - 1
q_icondj = Q / v2
q_ij = (q_icondj + t(q_icondj)) / (2 * n)
q_jcondi = t(q_icondj)
#Computing the gradient
#Para hacer la inversa de esta matriz utilizo la funcion solve
inversa = as.matrix((1+ (dist(ydata, diag = TRUE)) ^ 2))
inversa = solve(inversa)
#ARREGLAR EL GRADIENTE
gradient = matrix(data = 0, nrow = n, ncol = q)
# tot = vector(mode = "list", length = n)
# for(i in tot) {
#  gradient[i,] = 4 * colSums( (p_ij - q_ij) * (ydata - ydata[,i]) * inversa)
# }
#Probar de esta forma
#apply con 1 es row
#aaply con 2 es column
stiffnesses = 4 * (p_ij - q_ij) * inversa
#El problema está en esta funcion
#No actualiza bien el gradiente de manera que no se repelen bien los puntos
#Para coger las columnas y filas de ydata:
#Mirar documentacion pero como en matlab: ydata[1,]
#Once we have computed everything:
#Dejo directamente la del comentario
Y_t =  ydata
Y_t_1 = ydata
Y_t_2 = ydata
#Ycalc1 is Ycalc in the previous step
#ycalc2 is Ycalc 2 steps before
#Te = vector(mode = "list", length = 100)
Te = as.matrix(1:1000)
for (t in Te) {
for (i in 1:n){
gradient[i,] = apply(sweep(-Y_t, 2, -Y_t[i,]) * stiffnesses[,i] ,2,sum)
}
Y_t = Y_t_1 + learningrate * gradient + momentum * (Y_t_1 - Y_t_2)
#Ycalc = Ycalc1 + 100 * gradient + 0.8 * (Ycalc1 - Ycalc2)
#learningrate = learningrate +1;
Y_t_2 = Y_t_1
Y_t_1 = Y_t
message("Iteration #",t)
if((i%%10)==0){
plot(Y_t, pch =15 , col = iris$Species)
}
}
n <- 200
set.seed(12345)
mu <- c(2,2)
Sigma1 <- diag(c(2, 1))
Sigma2 <- rbind(c(1, -0.5), c(-0.5, 2))
samp <- rbind(mvtnorm::rmvnorm(n = n / 2, mean = mu, sigma = Sigma1),
mvtnorm::rmvnorm(n = n / 2, mean = -mu, sigma = Sigma2))
group_labels <- c(rep(1, n), rep(2, n))
plot(samp, col = group_labels)
Sigma1 <- diag(c(2, 1))
Sigma2 <- rbind(c(1, -0.5), c(-0.5, 2))
samp <- rbind(mvtnorm::rmvnorm(n = n / 2, mean = mu, sigma = Sigma1),
mvtnorm::rmvnorm(n = n / 2, mean = -mu, sigma = Sigma2))
?mvtnorm
library('mvtnorm')
install.packages('mvtnorm')
library('mvtnorm')
Sigma1 <- diag(c(2, 1))
Sigma2 <- rbind(c(1, -0.5), c(-0.5, 2))
samp <- rbind(mvtnorm::rmvnorm(n = n / 2, mean = mu, sigma = Sigma1),
mvtnorm::rmvnorm(n = n / 2, mean = -mu, sigma = Sigma2))
Sigma1 <- diag(c(2, 1))
Sigma2 <- rbind(c(1, -0.5), c(-0.5, 2))
samp <- rbind(mvtnorm::rmvnorm(n = n / 2, mean = mu, sigma = Sigma1),
mvtnorm::rmvnorm(n = n / 2, mean = -mu, sigma = Sigma2))
group_labels <- c(rep(1, n), rep(2, n))
plot(samp, col = group_labels)
Sigma1 <- diag(c(2, 1))
Sigma2 <- rbind(c(1, -0.5), c(-0.5, 2))
samp <- rbind(mvtnorm::rmvnorm(n = n / 2, mean = mu, sigma = Sigma1),
mvtnorm::rmvnorm(n = n / 2, mean = -mu, sigma = Sigma2))
group_labels <- c(rep(1, n), rep(2, n))
plot(samp, col = group_labels)
Sigma1 <- diag(c(2, 1))
Sigma2 <- rbind(c(1, -0.5), c(-0.5, 2))
samp <- rbind(mvtnorm::rmvnorm(n = n / 2, mean = mu, sigma = Sigma1),
mvtnorm::rmvnorm(n = n / 2, mean = -mu, sigma = Sigma2))
group_labels <- c(rep(1, n), rep(2, n))
plot(samp, col = group_labels)
plot(samp, col = group_labels)
plot(samp, col = rainbow(3))
plot(samp, col = rainbow(group_labels))
data <- iris[,-5]
#Cargar la data = iris[,-5]
#Sin la columna de las especiess(nombres)
tsne<- function(data){
set.seed(12345)
x = data
n = nrow(data)
p = ncol(data)
#set initial parameters
learningrate = 100
#Establecemos estos valores iniciales
momentum  = 0.5
#momentum = 0.8
perplexity = 10
#Set initial configuration of the low dimensional
#data
#number of dimensions we want to reduce to.
sigma = 1/sqrt(2)
q = 2
ydata = matrix(data = rnorm(n = (n * p), mean = 0, sd = sigma), nrow = n, ncol = q)
#calculating the conditional probabilities in high dimension:
P = matrix(data = 0, nrow = n, ncol = p)
P = as.matrix(exp((-dist(x, diag = TRUE) ^ 2) / (2 * sigma ^ 2)), nrow = n, ncol = p)
#exceptuating the diagonal
v = rowSums(P) - 1
p_icondj = P / v
p_ij = (p_icondj + t(p_icondj)) / (2 * n)
p_jcondi = t(p_icondj)
# I have found a trouble computing the perplexity:
# Main trouble is computing the log of 0, since the diag is 0
uno = (p_jcondi) * log2(p_jcondi + 1)
entropy = -colSums(uno)
perplexity = 2 ^ (entropy)
#Este valor deberia dar entre 5 y 50 para un correcto desarrollo
perplexity = 30
#calculating the conditional probabilities in low dimension:
Q = matrix(data = 0, nrow = n, ncol = q)
Q = as.matrix(exp((-dist(ydata, diag = TRUE) ^ 2)), nrow = n, ncol = q)
#The 1 comes from the diagonal
v2 = rowSums(Q) - 1
q_icondj = Q / v2
q_ij = (q_icondj + t(q_icondj)) / (2 * n)
q_jcondi = t(q_icondj)
#Computing the gradient
#Para hacer la inversa de esta matriz utilizo la funcion solve
inversa = as.matrix((1+ (dist(ydata, diag = TRUE)) ^ 2), nrow = n, ncol = q)
inversa = solve(inversa)
gradient = matrix(data = 0, nrow = n, ncol = q)
# tot = vector(mode = "list", length = n)
# for(i in tot) {
#  gradient[i,] = 4 * colSums( (p_ij - q_ij) * (ydata - ydata[,i]) * inversa)
# }
#Probar de esta forma
#apply con 1 es row
#apply con 2 es column
stiffnesses = 4 * (p_ij - q_ij) * inversa
#Para coger las columnas y filas de ydata:
#Mirar documentacion pero como en matlab: ydata[1,]
#Once we have computed everything:
Y_t =  ydata
Y_t_1 = ydata
Y_t_2 = ydata
Te = as.matrix(1:1000)
for (t in Te) {
Q = as.matrix(exp((-dist(Y_t, diag = TRUE) ^ 2)), nrow = n, ncol = q)
#The 1 comes from the diagonal
v2 = rowSums(Q) - 1
q_icondj = Q / v2
q_ij = (q_icondj + t(q_icondj)) / (2 * n)
inversa = as.matrix((1+ (dist(Y_t, diag = TRUE)) ^ 2), nrow = n, ncol = q)
inversa1 = solve(inversa)
stiffnesses = 4 * (p_ij - q_ij) * inversa1
for (i in 1:n){
gradient[i,] = apply(sweep(Y_t, 2, -Y_t[i,]) * stiffnesses[,i] ,2,sum)
}
Y_t = Y_t_1 + learningrate * gradient + momentum * (Y_t_1 - Y_t_2)
if(t==1){
plot(Y_t, pch = 15 , col = iris$Species)
}
if(t > 700){
#learningrate = learningrate - 1
#Mayor learning rate menor los resultados, menor learning mas grandes
momentum = 0.8
}
Y_t_2 = Y_t_1
Y_t_1 = Y_t
message("Iteration #",t)
if((t%%100)==0){
plot(Y_t, pch = 15 , col = iris$Species)
}
}
Y_t
return(Y_t)
}
tsne1 <- tsne(data)
tsne1 <- tsne(data)
?scatter3e
?scatter3d
?? scatter3d
scatter3D()
